# [ICML-2022] Rethinking Attention-Model Explainability through Faithfulness Violation Test

Please see the detailed running steps in subfolders.

## Citation
If you make use of our work, please cite our paper:

    @InProceedings{Liu_2022_ICML,
    author = {Liu, Yibing and Li, Haoliang and Guo, Yangyang and Kong, Chenqi and Li, Jing and Wang, Shiqi},
    title = {Rethinking Attention-Model Explainability through Faithfulness Violation Test},
    year = {2022},
    booktitle = {{ICML}},
    }


## Credits
This work is built based on the implementation of 2021-ICCV work [Generic Attention-Model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transform](https://github.com/hila-chefer/Transformer-MM-Explainability), and 2020-ACL work [Towards Transparent and Explainable Attention Models](https://github.com/akashkm99/Interpretable-Attention). Many thanks for the generous sharing.